{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Fisher Information\n",
    "\n",
    "### Carolyn P. Johnston, March 10th, 2016\n",
    "\n",
    "Fisher Information is a quantity from mathematical statistics that is a characteristic of a random variable that is parametrized by a parameter $\\theta$. Fisher Information is defined as: \n",
    "\n",
    "$$I(\\theta) = \\text{E}_Y[{(\\frac{\\delta}{\\delta\\theta}}\\text{log}(f_\\theta(Y)))^2].$$\n",
    "\n",
    "The Fisher Information is important because it crops up as the multiplier in the statement of the Cramer-Rao theorem, which gives a lower bound on the variance of any possible *unbiased* estimator of a parameter: \n",
    "\n",
    "$$\\text{Var}(\\hat\\theta) \\ge \\frac{1}{I(\\theta)}.$$\n",
    "\n",
    "This is an important result, because it is a firm lower bound on the accuracy with which you can estimate a parameter if you want to stick with unbiased estimators. For example, if you want to estimate the Bernoulli parameter $\\phi$ for a coin toss by dividing the number of heads by the total number of tosses (which is an unbiased estimator for $\\phi$), the Fisher Information in that case is $n/{\\phi(1-\\phi)}$, and so the theoretical variance of this estimator is bounded below by ${\\phi(1-\\phi)}/n$ (in fact, for this sample mean estimator, that is its exact variance). I think of Cramer-Rao as a sort of 'Heisenberg inequality' for unbiased estimation.  \n",
    "\n",
    "If you like seeing things proven, you can look up an informal derivation of this result [here](https://en.wikipedia.org/wiki/Fisher_information#Informal_derivation_of_the_Cram.C3.A9r.E2.80.93Rao_bound \"Informal Derivation of CR bound\"). It uses well-known results from calculus, such as the product rule and the Cauchy-Schwarz inequality.\n",
    "\n",
    "I recently developed a bit of an obsession about Fisher Information, because while I could see clearly how it arises naturally in the derivation of the Cramer-Rao bound, and while I had calculated lots of examples of Fisher Information for all the basic distributions, I didn't really get why it was innately *informative*.\n",
    "\n",
    "If you play around a bit, it's not an unreasonable candidate for an information metric. For example, the Fisher Information for the mean $\\mu$ contained in one observation of a Gaussian with mean $\\mu$, and known variance $\\sigma^2$, is $1/\\sigma^2$. If a Gaussian is known to have large variance, then observing a single value drawn from it does not tell you much about the value of $\\mu$; but if its variance is small, then a single value tells you a lot about $\\mu$.\n",
    "\n",
    "Here is an angle by which I finally grabbed onto the idea a bit. Consider the definition\n",
    "\n",
    "$$I(\\theta) = \\text{E}_Y[({\\frac{\\delta}{\\delta\\theta}}\\text{log}(f_\\theta(Y)))^2] = \\int ({\\frac{\\delta}{\\delta\\theta}}\\text{log}(f_\\theta(Y)))^2 f_\\theta(Y) dY.$$\n",
    "\n",
    "A Monte Carlo estimator for that Fisher Information integral would be:\n",
    "\n",
    "$$F_N = \\int ({\\frac{\\delta}{\\delta\\theta}}\\text{log}(f_\\theta(Y)))^2 f_\\theta(Y) dY \\approx \\frac{1}{N}\\sum_{i=1}^N ({\\frac{\\delta}{\\delta\\theta}}\\text{log}(f_\\theta(y_i)))^2,$$\n",
    "\n",
    "where $y_i$ is a sample of N draws from the distribution of $f_\\theta$. By the law of large numbers, this must converge to the Fisher Information as N gets large. But compare it to this quantity:\n",
    "\n",
    "$$\\frac{1}{N}\\sum_i {\\frac{\\delta}{\\delta\\theta}}\\text{log}(f_\\theta(y_i)).$$\n",
    "\n",
    "This is the derivative of the log-likelihood of the data with respect to $\\theta$ -- exactly the term that we set to 0, and solve, in order to find the MLE of $\\theta$. The only difference between this sum of 1st derivatives, and the Fisher Information sum $F_N$, is that the terms in $F_N$ are squared.\n",
    "\n",
    "So, we can think of the Fisher Information definition as consisting of squares of the score function (1st derivative of the log-likelihood function), summed over the samples. The derivatives themselves sum to 0 at the MLE, and since the MLE is consistent (i.e., unbiased in the limit), then as N gets large we expect the MLE to get closer to the actual value of $\\theta$.\n",
    "\n",
    "There are two ways that a sum of signed values can be zero; either the terms themselves are all small in absolute value (in which case the likelihood curve is not changing much as a function of $\\theta, and the Fisher information is small -- think of a Gaussian with large variance, so that the log-likelihood is a broad parabola), or some are large in absolute value and a lot of cancellation is occurring in the sum (think of a Gaussian with small variance, which has a narrow-peaked log-likelihood). This latter is what happens in the large Fisher-Information situation. Because the absolute values of some of the score terms in the large Fisher-Information case are relatively large, a new observation has the potential to change the sum of score functions quite a bit -- i.e., every new observation adds information.   \n",
    "\n",
    "There is also another expression for Fisher Information, as the negative expected value of the 2nd derivative of the log-likelihood -- some calculus shows that they are equivalent, but I never came up with a 'vision' for understanding Fisher Information, using that definition, that was satisfying to me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
