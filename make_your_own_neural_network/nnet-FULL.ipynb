{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FULL training run, long enough to observe overfitting.\n",
    "\n",
    "Coded with the guidance of 'Make your own neural network'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "#neural network class definition \n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    # input number of nodes in each of 3 layers\n",
    "    def __init__(self, ninodes, nhnodes, nonodes, lr):\n",
    "        \n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        self.inodes = ninodes\n",
    "        self.hnodes = nhnodes\n",
    "        self.onodes = nonodes\n",
    "        self.lr = lr\n",
    "        \n",
    "        # initialize matrix with normal random variables \n",
    "        self.wih = np.random.normal(0.0, \n",
    "                                        pow(float(self.hnodes), -0.5),\n",
    "                                        (self.hnodes, self.inodes)\n",
    "                                      )\n",
    "        self.who = np.random.normal(0.0,\n",
    "                                     pow(float(self.onodes), -0.5),\n",
    "                                     (self.onodes, self.hnodes)\n",
    "                                      )\n",
    "        \n",
    "        \n",
    "    def train(self, inputs_list, targets_list):\n",
    "        \n",
    "        # if inputs_list is a list of m items each of length n,\n",
    "        # np.array will turn it into an mxn matrix.\n",
    "        # ie, it will add records row by row.\n",
    "        # the result gets transposed here.\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        # calculate signals coming out of hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals going into output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        # calculate final output\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        \n",
    "        # We will need the output and the hidden layer errors,\n",
    "        # both calculated up front, i.e., before the changes are applied to the interior weights.\n",
    "        # the hidden layer error is equal to the output_errors \n",
    "        # passed backward through the weights between the hidden and output\n",
    "        # layers (transposed).\n",
    "        # note that self.wts_ho.T is num_hidden x num_output,\n",
    "        # so hidden_errors is a num_hidden x 1 vector.\n",
    "        output_errors = targets - final_outputs\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        \n",
    "        # now we backpropagate.\n",
    "        # step 1: adjust the hidden->outer weights wts_ho.\n",
    "        # these are components of the gradient of the error function\n",
    "        # wrt the hidden->output weights wts_ho (dimension outer x hidden).\n",
    "        # note that this is an outer product of the \n",
    "        # hidden and output components of the gradient        \n",
    "        self.who += self.lr * np.dot((output_errors*final_outputs*(1.0-final_outputs)), hidden_outputs.T)\n",
    "        \n",
    "        # the next step is the same, shifted to the input->hidden weights.\n",
    "        #hidden_component_of_wts_ih_delta = hidden_errors*hidden_outputs*(1.0-hidden_outputs)\n",
    "        #input_component_of_wts_ih_delta = inputs\n",
    "        \n",
    "        # again, outer product of input and hidden components of the gradient\n",
    "        self.wih += self.lr * np.dot((hidden_errors*hidden_outputs*(1.0-hidden_outputs)), inputs.T) \n",
    "        \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def query(self,inputs_list):\n",
    "        inputs = np.array(inputs_list, ndmin =2).T\n",
    "        \n",
    "        # calcualte signals entering hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        # signals leaving hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # signals entering final layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        # signals leaving final layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 784\n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# create instance of neural network\n",
    "n = NeuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now having gotten the neural net running in nnet1.ipynb, train the neural net on the full MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/xferUbuntu/notebooks/make_your_own_neural_network\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mnist_test.csv',\n",
       " 'mnist_test_10.csv',\n",
       " 'mnist_train.csv',\n",
       " 'mnist_train_100.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/mnt/xferUbuntu/notebooks/make_your_own_neural_network/mnist\")\n",
    "os.listdir(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_file = open(os.path.join(os.getcwd(), \"mnist_train.csv\"), 'r')\n",
    "training_data = training_file.readlines()\n",
    "training_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carolyn/anaconda/envs/notebk/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural net using the MNIST data, and 200 hidden nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inodes = 28*28 # number of input nodes = dimensions of input images\n",
    "hnodes = 200 # he starts by using 100 hidden nodes\n",
    "onodes = 10 # number of output nodes = 10 (1 per digit)\n",
    "\n",
    "training_list = []\n",
    "target_list = []\n",
    "lrate = 0.01\n",
    "nnet2 = NeuralNetwork(inodes, hnodes, onodes, lrate)\n",
    "\n",
    "for line in training_data:\n",
    "    all_values = line.split(',')\n",
    "    assert len(all_values) == 28*28+1\n",
    "    \n",
    "    # rescale the input values into the range [0.01, 1.0]\n",
    "    # you're avoiding the value 0 because the weights can't affect that value,\n",
    "    # but the value 1 is okay for input.\n",
    "    # note you do not need to reshape the array before using it for training...\n",
    "    # the neural net doesn't care whether it looks like an image to you!\n",
    "    inputs = ((np.asfarray(all_values[1:])/255.0)*0.99) + 0.01\n",
    "    assert len(inputs) == inodes\n",
    "    assert inputs.min() >0\n",
    "    \n",
    "    # all the non-target entries will be .01, the target entry will be .99\n",
    "    # otherwise you'll get large weights and a saturated network because you'll be trying \n",
    "    # to hit impossible values\n",
    "    targets = (np.zeros(onodes) + 0.01)\n",
    "    targets[int(all_values[0])] = 0.99\n",
    "\n",
    "    # ===>>> now we train individually with every sample.\n",
    "    nnet2.train(inputs, targets)\n",
    "\n",
    "# NOTE: BEFORE I FOUND THIS BUG, I WAS TRAINING WITH EVERYTHING in one pass, \n",
    "# instead of iteratively, as below:\n",
    "# nnet2.train(training_list, target_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now test the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open and set up the training data. \n",
    "test_file = open(os.path.join(os.getcwd(), \"mnist_test.csv\"), 'r')\n",
    "test_data = test_file.readlines()\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = []\n",
    "test_target_indices = []\n",
    "for line in test_data:\n",
    "    all_values = line.split(',')\n",
    "    \n",
    "    # rescale the input values into the range [0.01, 1.0]\n",
    "    # you're avoiding the value 0 because the weights can't affect that value,\n",
    "    # but the value 1 is okay for input.\n",
    "    # note you do not need to reshape the array before using it for training...\n",
    "    # the neural net doesn't care whether it looks like an image to you!\n",
    "    image_array = (np.asfarray(all_values[1:])/255.0*0.99) +0.01\n",
    "    \n",
    "    # rescale the output targets into the range [.01, .99]\n",
    "    # all the non-target entries will be .01, the target entry will be .99\n",
    "    # otherwise you'll get large weights and a saturated network because you'll be trying \n",
    "    # to hit impossible values (0, 1)\n",
    "    target_array = (np.zeros(onodes)+0.01)\n",
    "    target_index = int(all_values[0])\n",
    "    target_array[target_index]=0.99\n",
    "\n",
    "    # append the training sample\n",
    "    test_list.append(image_array)\n",
    "    test_target_indices.append(target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network says: 7, Target was 7\n",
      "Network says: 2, Target was 2\n",
      "Network says: 1, Target was 1\n",
      "Network says: 0, Target was 0\n",
      "Network says: 4, Target was 4\n",
      "Network says: 1, Target was 1\n",
      "Network says: 4, Target was 4\n",
      "Network says: 9, Target was 9\n",
      "Network says: 6, Target was 5\n",
      "Network says: 9, Target was 9\n",
      "Network says: 0, Target was 0\n",
      "Network says: 6, Target was 6\n",
      "Accuracy with 1 epoch: 0.916666666667\n"
     ]
    }
   ],
   "source": [
    "# get test statistics\n",
    "scorecard = []\n",
    "margins = []\n",
    "# go through test records\n",
    "for ii, record in enumerate(test_list):\n",
    "    \n",
    "    results = nnet2.query(record)\n",
    "    label = np.argmax(results)\n",
    "    print \"Network says: {}, Target was {}\".format(label, test_target_indices[ii])\n",
    "    if label == test_target_indices[ii]:\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        scorecard.append(0)\n",
    "    if ii > 10:\n",
    "        break\n",
    "        \n",
    "print \"Accuracy with 1 epoch: {}\".format(np.mean(scorecard))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with learning rate .01, accuracy is 91 2/3%\n",
    "#### now what is it like if I increase the number of epochs? I.e., do more than one pass through the training data. \n",
    "\n",
    "#### I use the same code, but now I wrap it in an outer loop. \n",
    "### Note that even after 10 epochs, I'm not overfitting yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 0 epochs: 0.9238\n",
      "Accuracy with 1 epochs: 0.9405\n",
      "Accuracy with 2 epochs: 0.9518\n",
      "Accuracy with 3 epochs: 0.9599\n",
      "Accuracy with 4 epochs: 0.9639\n",
      "Accuracy with 5 epochs: 0.9656\n",
      "Accuracy with 6 epochs: 0.9671\n",
      "Accuracy with 7 epochs: 0.9686\n",
      "Accuracy with 8 epochs: 0.9702\n",
      "Accuracy with 9 epochs: 0.971\n"
     ]
    }
   ],
   "source": [
    "training_list = []\n",
    "target_list = []\n",
    "lrate = 0.01\n",
    "nnet3 = NeuralNetwork(inodes, hnodes, onodes, lrate)\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    for line in training_data:\n",
    "        all_values = line.split(',')\n",
    "        assert len(all_values) == 28*28+1\n",
    "\n",
    "        # rescale the input values into the range [0.01, 1.0]\n",
    "        # you're avoiding the value 0 because the weights can't affect that value,\n",
    "        # but the value 1 is okay for input.\n",
    "        # note you do not need to reshape the array before using it for training...\n",
    "        # the neural net doesn't care whether it looks like an image to you!\n",
    "        inputs = ((np.asfarray(all_values[1:])/255.0)*0.99) + 0.01\n",
    "        assert len(inputs) == inodes\n",
    "        assert inputs.min() >0\n",
    "\n",
    "        # all the non-target entries will be .01, the target entry will be .99\n",
    "        # otherwise you'll get large weights and a saturated network because you'll be trying \n",
    "        # to hit impossible values\n",
    "        targets = (np.zeros(onodes) + 0.01)\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "\n",
    "        # ===>>> now we train individually with every sample.\n",
    "        nnet3.train(inputs, targets)\n",
    "\n",
    "    # tuck the test loop inside the epochs loop, \n",
    "    # so you get a test rate per epoch. \n",
    "    scorecard = []\n",
    "    margins = []\n",
    "    \n",
    "    # go through test records\n",
    "    for ii, record in enumerate(test_list):\n",
    "\n",
    "        results = nnet3.query(record)\n",
    "        label = np.argmax(results)\n",
    "        #print \"Network says: {}, Target was {}\".format(label, test_target_indices[ii])\n",
    "        if label == test_target_indices[ii]:\n",
    "            scorecard.append(1)\n",
    "        else:\n",
    "            scorecard.append(0)\n",
    "\n",
    "    accuracy = np.mean(scorecard)\n",
    "    accuracies.append(accuracy)\n",
    "    print \"Accuracy with {} epochs: {}\".format(epoch, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 11 epochs: 0.9717\n",
      "Accuracy with 12 epochs: 0.9735\n",
      "Accuracy with 13 epochs: 0.974\n",
      "Accuracy with 14 epochs: 0.9746\n",
      "Accuracy with 15 epochs: 0.9747\n",
      "Accuracy with 16 epochs: 0.9748\n",
      "Accuracy with 17 epochs: 0.9754\n",
      "Accuracy with 18 epochs: 0.9757\n",
      "Accuracy with 19 epochs: 0.976\n",
      "Accuracy with 20 epochs: 0.9767\n",
      "Accuracy with 21 epochs: 0.9772\n",
      "Accuracy with 22 epochs: 0.9774\n",
      "Accuracy with 23 epochs: 0.978\n",
      "Accuracy with 24 epochs: 0.9779\n",
      "Accuracy with 25 epochs: 0.9779\n",
      "Accuracy with 26 epochs: 0.9778\n",
      "Accuracy with 27 epochs: 0.978\n",
      "Accuracy with 28 epochs: 0.9781\n",
      "Accuracy with 29 epochs: 0.9783\n",
      "Accuracy with 30 epochs: 0.9784\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10, 30):\n",
    "    for line in training_data:\n",
    "        all_values = line.split(',')\n",
    "        assert len(all_values) == 28*28+1\n",
    "\n",
    "        # rescale the input values into the range [0.01, 1.0]\n",
    "        # you're avoiding the value 0 because the weights can't affect that value,\n",
    "        # but the value 1 is okay for input.\n",
    "        # note you do not need to reshape the array before using it for training...\n",
    "        # the neural net doesn't care whether it looks like an image to you!\n",
    "        inputs = ((np.asfarray(all_values[1:])/255.0)*0.99) + 0.01\n",
    "        assert len(inputs) == inodes\n",
    "        assert inputs.min() >0\n",
    "\n",
    "        # all the non-target entries will be .01, the target entry will be .99\n",
    "        # otherwise you'll get large weights and a saturated network because you'll be trying \n",
    "        # to hit impossible values\n",
    "        targets = (np.zeros(onodes) + 0.01)\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "\n",
    "        # ===>>> now we train individually with every sample.\n",
    "        nnet3.train(inputs, targets)\n",
    "\n",
    "    # tuck the test loop inside the epochs loop, \n",
    "    # so you get a test rate per epoch. \n",
    "    # Keep adding to the scorecard array of accuracies.\n",
    "    scorecard = []\n",
    "    \n",
    "    # go through test records\n",
    "    for ii, record in enumerate(test_list):\n",
    "\n",
    "        results = nnet3.query(record)\n",
    "        label = np.argmax(results)\n",
    "        #print \"Network says: {}, Target was {}\".format(label, test_target_indices[ii])\n",
    "        if label == test_target_indices[ii]:\n",
    "            scorecard.append(1)\n",
    "        else:\n",
    "            scorecard.append(0)\n",
    "\n",
    "    accuracy = np.mean(scorecard)\n",
    "    accuracies.append(accuracy)\n",
    "    print \"Accuracy with {} epochs: {}\".format(epoch+1, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after 30 epochs, still not overfitting. The learning rate, .01, is low compared to what is recommended in the text (0.2). That is probably why. But consider this comment in the text:\n",
    "\n",
    "Intuitively it makes sense that if you plan to explore the gradient descent for much longer (more epochs), you can afford to take shorter steps (learning rate), and overall youâ€™ll find a better path down.\n",
    "\n",
    "### His tests top out at roughly .97, and I'm approaching a .98. Could I be finding a better path because my learning rate is lower? I'll run it for a while longer and see if I top out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 31 epochs: 0.9786\n",
      "Accuracy with 32 epochs: 0.9789\n",
      "Accuracy with 33 epochs: 0.9788\n",
      "Accuracy with 34 epochs: 0.9788\n",
      "Accuracy with 35 epochs: 0.9785\n",
      "Accuracy with 36 epochs: 0.9786\n",
      "Accuracy with 37 epochs: 0.9786\n",
      "Accuracy with 38 epochs: 0.9785\n",
      "Accuracy with 39 epochs: 0.9787\n",
      "Accuracy with 40 epochs: 0.9788\n",
      "Accuracy with 41 epochs: 0.979\n",
      "Accuracy with 42 epochs: 0.979\n",
      "Accuracy with 43 epochs: 0.979\n",
      "Accuracy with 44 epochs: 0.9787\n",
      "Accuracy with 45 epochs: 0.9787\n",
      "Accuracy with 46 epochs: 0.9788\n",
      "Accuracy with 47 epochs: 0.9789\n",
      "Accuracy with 48 epochs: 0.9788\n",
      "Accuracy with 49 epochs: 0.9785\n",
      "Accuracy with 50 epochs: 0.9784\n",
      "Accuracy with 51 epochs: 0.978\n",
      "Accuracy with 52 epochs: 0.9779\n",
      "Accuracy with 53 epochs: 0.9782\n",
      "Accuracy with 54 epochs: 0.9782\n",
      "Accuracy with 55 epochs: 0.9783\n",
      "Accuracy with 56 epochs: 0.9785\n",
      "Accuracy with 57 epochs: 0.9787\n",
      "Accuracy with 58 epochs: 0.9784\n",
      "Accuracy with 59 epochs: 0.9783\n",
      "Accuracy with 60 epochs: 0.9781\n",
      "Accuracy with 61 epochs: 0.978\n",
      "Accuracy with 62 epochs: 0.9782\n",
      "Accuracy with 63 epochs: 0.9781\n",
      "Accuracy with 64 epochs: 0.9779\n",
      "Accuracy with 65 epochs: 0.978\n",
      "Accuracy with 66 epochs: 0.9778\n",
      "Accuracy with 67 epochs: 0.9779\n",
      "Accuracy with 68 epochs: 0.9778\n",
      "Accuracy with 69 epochs: 0.978\n",
      "Accuracy with 70 epochs: 0.9777\n",
      "Accuracy with 71 epochs: 0.9775\n",
      "Accuracy with 72 epochs: 0.9776\n",
      "Accuracy with 73 epochs: 0.9777\n",
      "Accuracy with 74 epochs: 0.9775\n",
      "Accuracy with 75 epochs: 0.9778\n",
      "Accuracy with 76 epochs: 0.9775\n",
      "Accuracy with 77 epochs: 0.9775\n",
      "Accuracy with 78 epochs: 0.9774\n",
      "Accuracy with 79 epochs: 0.9778\n",
      "Accuracy with 80 epochs: 0.9778\n",
      "Accuracy with 81 epochs: 0.978\n",
      "Accuracy with 82 epochs: 0.9782\n",
      "Accuracy with 83 epochs: 0.9782\n",
      "Accuracy with 84 epochs: 0.9781\n",
      "Accuracy with 85 epochs: 0.9783\n",
      "Accuracy with 86 epochs: 0.9785\n",
      "Accuracy with 87 epochs: 0.978\n",
      "Accuracy with 88 epochs: 0.9778\n",
      "Accuracy with 89 epochs: 0.9778\n",
      "Accuracy with 90 epochs: 0.978\n",
      "Accuracy with 91 epochs: 0.9781\n",
      "Accuracy with 92 epochs: 0.9781\n",
      "Accuracy with 93 epochs: 0.9779\n",
      "Accuracy with 94 epochs: 0.978\n",
      "Accuracy with 95 epochs: 0.978\n",
      "Accuracy with 96 epochs: 0.9779\n",
      "Accuracy with 97 epochs: 0.9777\n",
      "Accuracy with 98 epochs: 0.9777\n",
      "Accuracy with 99 epochs: 0.978\n",
      "Accuracy with 100 epochs: 0.9778\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30, 100):\n",
    "    for line in training_data:\n",
    "        all_values = line.split(',')\n",
    "        assert len(all_values) == 28*28+1\n",
    "\n",
    "        # rescale the input values into the range [0.01, 1.0]\n",
    "        # you're avoiding the value 0 because the weights can't affect that value,\n",
    "        # but the value 1 is okay for input.\n",
    "        # note you do not need to reshape the array before using it for training...\n",
    "        # the neural net doesn't care whether it looks like an image to you!\n",
    "        inputs = ((np.asfarray(all_values[1:])/255.0)*0.99) + 0.01\n",
    "        assert len(inputs) == inodes\n",
    "        assert inputs.min() >0\n",
    "\n",
    "        # all the non-target entries will be .01, the target entry will be .99\n",
    "        # otherwise you'll get large weights and a saturated network because you'll be trying \n",
    "        # to hit impossible values\n",
    "        targets = (np.zeros(onodes) + 0.01)\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "\n",
    "        # ===>>> now we train individually with every sample.\n",
    "        nnet3.train(inputs, targets)\n",
    "\n",
    "    # tuck the test loop inside the epochs loop, \n",
    "    # so you get a test rate per epoch. \n",
    "    # Keep adding to the scorecard array of accuracies.\n",
    "    scorecard = []\n",
    "    \n",
    "    # go through test records\n",
    "    for ii, record in enumerate(test_list):\n",
    "\n",
    "        results = nnet3.query(record)\n",
    "        label = np.argmax(results)\n",
    "        #print \"Network says: {}, Target was {}\".format(label, test_target_indices[ii])\n",
    "        if label == test_target_indices[ii]:\n",
    "            scorecard.append(1)\n",
    "        else:\n",
    "            scorecard.append(0)\n",
    "\n",
    "    accuracy = np.mean(scorecard)\n",
    "    accuracies.append(accuracy)\n",
    "    print \"Accuracy with {} epochs: {}\".format(epoch+1, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Note that accuracy topped out at .9789 at 47 epochs. Unfortunately I can't plot epoch v. accuracy since Ubuntu crashed at some point, but it appears that after that point, the neural net began overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
