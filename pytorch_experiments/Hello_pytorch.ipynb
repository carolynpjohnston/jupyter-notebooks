{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6720  0.7877  0.7345\n",
      " 0.1567  0.4794  0.3279\n",
      " 0.1312  0.0936  0.4808\n",
      " 0.7903  0.5292  0.6350\n",
      " 0.9796  0.6047  0.4758\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Any operation that mutates a tensor in-place is post-fixed with an underscore. For example: 'x.copy_(y)', and 'x.t_()', will change x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = torch.zeros(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6720  0.7877  0.7345\n",
      " 0.1567  0.4794  0.3279\n",
      " 0.1312  0.0936  0.4808\n",
      " 0.7903  0.5292  0.6350\n",
      " 0.9796  0.6047  0.4758\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  0  0\n",
      " 0  0  0\n",
      " 0  0  0\n",
      " 0  0  0\n",
      " 0  0  0\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.copy_(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Standard numpy indexing works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In fact, you can convert back and forth from a Torch array to a numpy ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = x.numpy()\n",
    "print(b)\n",
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      " 1  1  1  1  1\n",
      "[torch.DoubleTensor of size 3x5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.DoubleTensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones((3,5))\n",
    "c = torch.from_numpy(a)\n",
    "print(c)\n",
    "type(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "At the moment, CUDA is unavailable. But you can have blocks of code that run only if it is. This call moves tensors onto the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"C is available\")\n",
    "    x = x.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Autograd: automatic differentiation exercise\n",
    "\n",
    "the autograd package provides automatic differentiation for all operations on Tensors. \n",
    "\n",
    "autograd.Variable is the central class of this package; it wraps a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(2,2),requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "<torch.autograd._functions.basic_ops.AddConstant object at 0x7f6cf03d5ba8>\n"
     ]
    }
   ],
   "source": [
    "# every Variable object has a creator attribute which points\n",
    "# back to the function that created it. I guess that helps \n",
    "# track backward for differentiation.\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### It looks like the creator is always the last function in the expression that created it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = y*y*3\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can always access the underlying torch tensor through the .data accessor.\n",
    "type(y.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I am not yet sure what out.backward does.\n",
    "out.backward(retain_variables=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the 'out' variable is O, then $O={1\\over 4}\\Sigma_i z_i$, and $z_i=3(x_i + 2)^2$. \n",
    "\n",
    "So,\n",
    "\n",
    "${dO \\over dx_i} = {dO \\over dz_i}\\cdot {dz_i \\over dx_i} = \n",
    "{1\\over 4}\\cdot 6(x_i + 2) = {18 \\over 4} = 4.5$ \n",
    "\n",
    "if the expression is evaluated at $x_i = 1$.\n",
    "\n",
    "So it seems that the grad function gives the derivative of variables deeper in the computation graph, with respect to the calling variable.  But then shouldn't z.grad be defined as well? It's still 'None'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another autograd example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  853.5403\n",
      "  327.2983\n",
      " 1259.5571\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize a random variable\n",
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "\n",
    "# multiply it by 2 until its norm grows to a certain size\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The meaning of the 'gradients' parameter which is passed to 'backward'\n",
    "\n",
    "Now you can take its derivative with respect to  the initial 3-vector x\n",
    "I pass it the derivative at y: dZ/dy = (1,1,1)\n",
    "\n",
    "It chokes if you don't feed it the 'gradients' term which has to have the same dimensionality as the output.\n",
    "\n",
    "NTS: After reading up on automatic differentiation -- at the end of the  graph, there is an implicit single Z output value. Backpropagation actually computes dZ/dx for every x in the graph. So if you are considering a multidimensional output (y1,y2, y3) for the graph, you need to initialize the backpropagation with dZ/dy1, dZ/dy2, and dZ/dy3. \n",
    "\n",
    "Also NTS: there is also an implicit dZ/dZ == 1 at the end of the graph, which is why, if your output is 1-dimensional, you can leave 'gradients' out of the PyTorch backward function and it will assume your last node is Z, and assume a starting value of dZ/dZ = 1.\n",
    "\n",
    "In this example, y is a 3-tensor so you have to give it an explicit gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1024\n",
      " 1024\n",
      " 1024\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now you can take its derivative with respect to  the initial 3-vector x\n",
    "# by default it takes the derivative at x = (1,1,1)\n",
    "# It chokes if you don't feed it the 'gradients' term which has to be 3D\n",
    "\n",
    "### NTS: After reading up on automatic differentiation -- at the end of the \n",
    "# graph, there has to be a single Z output value. \n",
    "# Backpropagation actually computes dZ/dx\n",
    "# for every x in the graph. So if you are considering a multidimensional output \n",
    "# (y1,y2, y3) for the graph, you are implicitly assuming you've started off \n",
    "# with dZ/dy1, dZ/dy2, and dZ/dy3. \n",
    "\n",
    "# NTS: there is also an implicit dZ/dZ == 1 at the end of the graph, which is why,\n",
    "# if your output is 1-dimensional, you can leave 'gradients' out of the \n",
    "# backward function and backward will assume your last node is Z, so that using a \n",
    "# starting value of dZ/dZ = 1 is appropriate.\n",
    "# In this example, y is a 3-tensor so you have to give it an explicit gradient.\n",
    "gradients = torch.FloatTensor([1,1,1])\n",
    "y.backward(gradients)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, don't use a trivial input gradient: make it nontrivial so you can see that the effect on the backpropagated gradient is just multiplicative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 81920.0000\n",
      "  8192.0000\n",
      "   819.2000\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize a random variable\n",
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "\n",
    "# multiply it by 2 until its norm grows to a certain size\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "gradients2 = torch.FloatTensor([10.0,1.0,.1])\n",
    "y.backward(gradients2)\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
