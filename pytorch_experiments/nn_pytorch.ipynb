{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is from the 60-minute blitz Pytorch tutorial, introducing the nn neural net module.\n",
    "\n",
    "The following code creates a neural net with the following layers:\n",
    "\n",
    "    Input: 32x32\n",
    "    C1: 6@28x28 -- by applying 6 5x5 convolutions\n",
    "    S2: 6@14x14 -- by subsampling (maxpooling?) each of the 6 channels\n",
    "    C3: 16@ 10x10 -- by applying 16 5x5 convolutions (? 16?)\n",
    "    S4: 16@ 5x5 -- subsampling each of the 16 channels\n",
    "    F5: fully connected layer, 120\n",
    "    F6: fully connected layer, 84\n",
    "    F7: fully connected layer, 10 (outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear (400 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # these are the parametrized components of the neural net.\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16 * 5 * 5, 120)  # an affine operation: y = Wx + b\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # backward is automatically defined using autograd!\n",
    "        # note the first two lines here actually combine\n",
    "        # convolution -> relu -> maxpool\n",
    "        # steps C1, S2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))  # Max pooling over a (2, 2) window\n",
    "        # steps C3, S4\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)  # If the size is a square you can only specify a single number\n",
    "        \n",
    "        # What does this do? I've no idea.\n",
    "        # oh wait -- it probably totally flattens the result of the last step.\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        # apply F5, relu\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # apply F6, relu\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # apply F7 and done\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The learnable parameters can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 10 parameter objects in the net\n",
      "The weights to layer C1 map from 1 input channel to 6 channels, each with 25 convolution parameters\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(\"there are {} parameter objects in the net\".format(len(params)))\n",
    "weightct = params[0].size()\n",
    "print(\"The weights to layer C1 map from {} input channel to {} channels, each with {} convolution parameters\".format(weightct[1], weightct[0], weightct[2]*weightct[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([6, 1, 5, 5])\n",
      "1 torch.Size([6])\n",
      "2 torch.Size([16, 6, 5, 5])\n",
      "3 torch.Size([16])\n",
      "4 torch.Size([120, 400])\n",
      "5 torch.Size([120])\n",
      "6 torch.Size([84, 120])\n",
      "7 torch.Size([84])\n",
      "8 torch.Size([10, 84])\n",
      "9 torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# I don't know what the extra parameter layers in there are doing, i.e,\n",
    "# params with size 6, 16, 120, 84, 10. \n",
    "# according to the text these all represent learnable parameters.\n",
    "# are they the biases? That could be it. Probably is.\n",
    "for ii in range(len(params)):\n",
    "    print(ii, params[ii].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Note about minibatches in pytorch nn.\n",
    "\n",
    "torch.nn only supports mini-batches The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.\n",
    "\n",
    "If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the net is a 4D tensor -- in this example it is a randomly generated single sample of the actual input 3D tensor -- a single channel 32x32 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.1191 -0.0391 -0.0911 -0.0144  0.1056  0.0148  0.0118 -0.0789  0.1334 -0.0505\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn((1,1,32,32)))\n",
    "\n",
    "# apparently you don't explicitly call 'forward'.\n",
    "out = net(input)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It is a bit weird how backpropagation works. \n",
    "\n",
    "First you zero out the gradient buffers of all the parameters. I assume it is these that will accumulate the backpropagated gradients.\n",
    "\n",
    "Then, since the output of the net is currently 10-dimensional, you have to give the net a 10-d gradient $dZ/dx_i$ to start with. In this case, you're making it random.\n",
    "\n",
    "Note that out.backward has no output itself.\n",
    "\n",
    "Note that the call out.backward() does not explicitly reference the network. So what is it doing? \n",
    "\n",
    "All of the Variables in the computation graph of 'out' (which means the weight variables in the neural net) are getting gradients computed and placed in their .grad attributes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You need to clear the gradients in each variable, or the new results \n",
    "# will be added to what's already there.\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7fadf97b2c50>\n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.1021 -0.0174  0.1215 -0.0019  0.0973\n",
      " -0.0035 -0.0146  0.1239 -0.1160 -0.0383\n",
      "  0.1767  0.1816  0.1941  0.0389  0.1270\n",
      " -0.0529  0.1996  0.0967  0.0255  0.1016\n",
      " -0.1507  0.0912  0.1385  0.1273 -0.1909\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.1432 -0.1984 -0.1351 -0.1715  0.0474\n",
      "  0.1262  0.0971 -0.0033 -0.0196 -0.1496\n",
      " -0.0555  0.1132 -0.1805  0.1554 -0.0678\n",
      " -0.0557 -0.0401  0.0557 -0.0195 -0.0014\n",
      " -0.1601  0.0202  0.0182  0.1517  0.0191\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      " -0.1816  0.0594  0.1996 -0.0621  0.1648\n",
      "  0.0702  0.0767 -0.1108  0.0961 -0.0520\n",
      " -0.1225 -0.1699 -0.0082 -0.0744  0.0446\n",
      "  0.1444  0.1849  0.0453  0.0824 -0.1156\n",
      "  0.0489 -0.0804 -0.1365  0.1102 -0.0840\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.0219 -0.0350  0.0316 -0.1324  0.1103\n",
      "  0.1659  0.1660 -0.0234 -0.1602 -0.0108\n",
      "  0.1611 -0.0371 -0.0661 -0.0128  0.1859\n",
      "  0.0217  0.0385 -0.0635  0.0217 -0.1827\n",
      "  0.0617  0.0964 -0.1648  0.1463  0.1507\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.1769  0.0607  0.0163 -0.0389  0.1747\n",
      "  0.1034  0.1485 -0.0594  0.0550 -0.1154\n",
      "  0.0683 -0.0726 -0.1816 -0.0273 -0.0717\n",
      " -0.1896  0.0080  0.1222 -0.1000  0.0224\n",
      " -0.1301  0.1915  0.1604  0.1989 -0.1959\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0818 -0.0546  0.0313 -0.1817 -0.0097\n",
      " -0.0339  0.1503  0.1719 -0.0639  0.0740\n",
      " -0.0854  0.1997  0.1358 -0.0169  0.1832\n",
      "  0.0762 -0.0141 -0.1496 -0.1523 -0.1297\n",
      "  0.0152 -0.1374  0.1052 -0.1269  0.0865\n",
      "[torch.FloatTensor of size 6x1x5x5]\n",
      "\n",
      "Parameter containing:\n",
      " 0.0602\n",
      "-0.1267\n",
      "-0.0449\n",
      "-0.0578\n",
      " 0.0747\n",
      " 0.0193\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to see the parameters themselves? \n",
    "# The implementation is a bit unclear.\n",
    "# I guess they implement it as a generator because it is a 4D tensor.\n",
    "# this printout confirms that the conv1 parameters do include an extra set of bias parameters.\n",
    "print(net.conv1.parameters())\n",
    "for param in net.conv1.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 38.4115\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "\n",
    "# this range target is a dummy target\n",
    "# again, the implementation is a bit awkward.\n",
    "# but the result is a Variable giving the loss function.\n",
    "# It looks like the MSELoss function call returns a function object which\n",
    "# you then call with the target and output.\n",
    "\n",
    "target = Variable(torch.range(1, 10))  \n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.nn._functions.thnn.auto.MSELoss object at 0x7fadf80ffac8>\n",
      "<torch.nn._functions.linear.Linear object at 0x7fadf80ff908>\n",
      "<torch.nn._functions.thnn.auto.Threshold object at 0x7fadf80ff828>\n"
     ]
    }
   ],
   "source": [
    "# now you are urged to follow the graph backward using creator.\n",
    "# MSELoss\n",
    "print(loss.creator) \n",
    "\n",
    "# What is Linear? Is it a matrix multiply? And if so why doesn't it follow ReLU?\n",
    "# Linear \n",
    "print(loss.creator.previous_functions[0][0])  \n",
    "# Relu\n",
    "print(loss.creator.previous_functions[0][0].previous_functions[0][0]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "conv1.bias.grad after backward\n",
      "Variable containing:\n",
      " 0.1713\n",
      " 0.0999\n",
      " 0.0469\n",
      "-0.1110\n",
      " 0.0136\n",
      "-0.0019\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In this step, we look at the gradient buffers before and after backprop.\n",
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight update\n",
    "\n",
    "The gradient buffers of the variables just contain the backpropped gradient data, it's up to the user to modify the weights using it.\n",
    "\n",
    "The method everyone uses is stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set a learning rate, run through all the parameters in the network, and\n",
    "# subtract a multiple of the gradient from every one.\n",
    "# recall that sub_ is postfixed with underscore because it subtracts \n",
    "# in place.\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim package\n",
    "\n",
    "From the tutorial:\n",
    "\n",
    "- However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer with learning rate \n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to be straightforward enough. the optimizer has the parameters, you do the forward and backprop, and then at the end the optimizer updates the parameters with whatever's in the gradient, in the appropriate way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
